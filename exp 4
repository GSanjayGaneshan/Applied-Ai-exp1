import numpy as np
def sigmoid(x):
return 1 / (1 + np.exp(-x))
def sigmoid_derivative(y):
return y * (1 - y)
def relu(x):
return np.maximum(0, x)
def relu_derivative(x):
return (x > 0).astype(float)
def leaky_relu(x, alpha=0.01):
return np.where(x > 0, x, alpha * x)
def leaky_relu_derivative(x, alpha=0.01):
return np.where(x > 0, 1, alpha)
def tanh(x):
return np.tanh(x)
def tanh_derivative(x):
return 1 - np.tanh(x) ** 2
def swish(x):
return x / (1 + np.exp(-x))
def swish_derivative(x):
sig = 1 / (1 + np.exp(-x))
return sig + x * sig * (1 - sig)
def softmax(x):
exp_x = np.exp(x - np.max(x))
return exp_x / np.sum(exp_x, axis=1, keepdims=True)
def softmax_derivative(softmax_output):
s = softmax_output.reshape(-1, 1)
return np.diagflat(s) - np.dot(s, s.T)
X = np.array([[1, 1, 0, 1]])
Y = np.array([[1]])
learning_rate = 0.07
threshold = 0.001
W1 = np.array([
[ 0.3, 0.1, -0.2],
[-0.2, 0.4, 0.3],
[ 0.2, -0.3, 0.1],
[ 0.1, 0.4, -0.1]
])
B1 = np.array([[0.2, 0.1, 0.05]])
W2 = np.array([
[ 0.3, -0.2],
[ 0.1, 0.4],
[-0.3, 0.2]
])
B2 = np.array([[0.1, -0.2]])
V = np.array([
[ 0.2],
[-0.3]
])
C = np.array([[0.1]])
epoch = 0
printed = False
while True:
epoch += 1
z1 = np.dot(X, W1) + B1
h1 = relu(z1)
z2 = np.dot(h1, W2) + B2
h2 = relu(z2)
u = np.dot(h2, V) + C
output = sigmoid(u)
error = Y - output
d_out = error * sigmoid_derivative(output)
d_h2 = d_out.dot(V.T) * relu_derivative(z2)
d_h1 = d_h2.dot(W2.T) * relu_derivative(z1)
V += learning_rate * h2.T.dot(d_out)
C += learning_rate * d_out
W2 += learning_rate * h1.T.dot(d_h2)
B2 += learning_rate * d_h2
W1 += learning_rate * X.T.dot(d_h1)
B1 += learning_rate * d_h1
if not printed:
print("\n--- First Epoch ---")
print("Output:", output.item())
print("Error:", error.item())
printed = True
if abs(error.item()) < threshold:
print("\n--- Last Epoch ---")
print("Epoch:", epoch)
print("Output:", output.item())
print("Error:", error.item())
break
